{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn0_TH-DtobK"
   },
   "source": [
    "**Neuro-AI: Harnessing AI to understand computation in mind and brain**  \n",
    "**Day 3: Neural encoding, decoding & reconstruction with deep learning**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuo7OGjyte8t"
   },
   "source": [
    "# Reconstructing handwritten digits from fMRI recordings\n",
    "\n",
    "In tutorial, you will implement a neural decoder for reconstructing perceived stimuli from brain responses.\n",
    "\n",
    "The dataset contains fMRI data acquired from the early visual cortex of one subject as the subject was presented with 100 grayscale images of handwritten sixes and nines (50 sixes and 50 nines). The fMRI data has been realigned and slice time corrected. Furthermore, stimulus specific response amplitudes have been estimated with a general linear model.\n",
    "\n",
    "Let's first familiarize ourselves with the dataset. It contains a number of variables:\n",
    "\n",
    "* **X** (stimuli) -> This is a 100 x 784 matrix. Each ith row contains the pixel values of a stimulus that was presented in the ith trial of the experiment. Note that the stimuli are 28 pixel x 28 pixel images, which were reshaped to 1 x 784 vectors.\n",
    "\n",
    "* **Y** (brain data) -> This is a 100 x 3092 matrix. The ith row contains the voxel values of the responses that were measured in the ith trial of the experiment.\n",
    "\n",
    "* **X_prior** (auxiliary images) -> This is a 2000 x 784 matrix. Each row contains the pixel values of a different stimulus, which was _not_ used in the experiment. Note that the stimuli are 28 pixel x 28 pixel images, which were reshaped to 1 x 784 vectors. You will be needing **X_prior** in task 3.\n",
    "\n",
    "Note: In the remainder of this document, we will use **x** for referring to a 784 x 1 stimulus vector and **y** for referring to a 3092 x 1 response vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ycLkNFbuMrf"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "* Load the dataset.\n",
    "* Visualize some of the stimuli.\n",
    "* Normalize X and Y and X_prior to have zero mean and unit variance. Tip: Recall that normalization means subtracting the mean of each pixel/voxel from itself and dividing it by its standard deviation. Also save these means and standard deviations as you will need them to denormalize the predictions again, for visualization purposes.\n",
    "* Split X and Y in two parts called X_training and X_test, and Y_training and Y_test. The training set should contain 80 stimulus-response pairs (40 pairs for sixes and 40 pairs for nines -- use the labels to evenly split these two categories). The test set should contain 20 stimulus-response pairs (10 pairs for sixes and 10 pairs for nines). Note that label 1 and 2 correspond to sixes and nines, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABehOjYk-8NB",
    "outputId": "e86dd5ae-c534-4f38-caa8-660768733849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-19 13:10:42--  https://github.com/tdado/NeuroAI24/raw/main/Basic/69dataset.mat\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/tdado/NeuroAI24/main/Basic/69dataset.mat [following]\n",
      "--2024-06-19 13:10:44--  https://raw.githubusercontent.com/tdado/NeuroAI24/main/Basic/69dataset.mat\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.28.133, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.28.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3220756 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘69dataset.mat’\n",
      "\n",
      "69dataset.mat       100%[===================>]   3.07M  4.17MB/s    in 0.7s    \n",
      "\n",
      "2024-06-19 13:10:46 (4.17 MB/s) - ‘69dataset.mat’ saved [3220756/3220756]\n",
      "\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'structural', 'funloc', 'masks', 'prior', 'labels', 'Y', 'X', 'description'])\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://github.com/tdado/NeuroAI24/raw/main/Basic/69dataset.mat\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "\n",
    "# Load the dataset\n",
    "data = scipy.io.loadmat(\"69dataset.mat\")\n",
    "print(data.keys())\n",
    "\n",
    "X = data[\"X\"]              # Shape: (100, 784)\n",
    "Y = data[\"Y\"]              # Shape: (100, 3092)\n",
    "X_prior = data[\"prior\"]    # Shape: (2000, 784)\n",
    "labels = data[\"labels\"]    # Shape: (100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSC3fwWormEe"
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA9jy9Y7uh4E"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Our goal is to solve the problem of reconstructing **x** from **y**. One possible approach to solve this problem is to use a *discriminative* model. Discriminative models predict **x** as a function of **y**. That is:\n",
    "\n",
    "$y = f(x)$\n",
    "\n",
    "We will assume that f is a linear function. That is:\n",
    "\n",
    "$x = B^\\top y$\n",
    "\n",
    "$f$ can be seen as a very simple linear neural network comprising one layer of weights (i.e., $B$). The aim of ridge regression is to minimize the objective function:\n",
    "\n",
    "$L(B) = \\|X - YB\\|^2 + \\lambda \\|B\\|^2$\n",
    "\n",
    "$B$ can be estimated in closed form:\n",
    "\n",
    "$B = (Y_{tr}^\\top Y_{tr} + \\lambda I)^{-1} Y_{tr}^\\top X_{tr}$\n",
    "\n",
    "where $\\lambda$ is the regularization coefficient, $I$ is the $q \\times q$ identity matrix, and $q$ is the number of voxels. Note that we can safely ignore the intercept since we normalized our data to have zero mean and unit variance.\n",
    "\n",
    "* Provide a derivation of the above closed-form solution for $B$ by setting the derivative of $L(B)$ to zero and find the optimal $B$.\n",
    "* Estimate $B$ on the training set. Tip: Normally, you should use cross validation to estimate lambda. For simplicity, you can assume that $\\lambda = 10^{-6}$.\n",
    "* Reconstruct $x$ from $y$ in the test set.\n",
    "* Visualize the reconstructions.\n",
    "\n",
    "ToThink 1: This model predicted the pixel values of an image by taking a linear combination of the stimulus-evoked brain responses. Does this imply that the relationship between responses and pixels of handwritten digits is inherently linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpAvywgCtcxm"
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FwZ1GpoupLe"
   },
   "source": [
    "## Task 3.1\n",
    "\n",
    "Another possible approach to solve the problem of reconstructing **x** from **y** is to use a *generative* model and invert it with Bayes' rule. We reformulate the problem as finding the most probable **x** that could have caused **y**. That is:\n",
    "\n",
    "argmax_**x** P(**x** | **y**)\n",
    "\n",
    "where P(**x** | **y**) is called the posterior (probability of the stimulus being **x** if the observation is **y**). In other words, we have to define the posterior, estimate its parameters and find the argument that maximizes it, which will be the reconstruction of **x** from **y**. While, this may seem daunting, it actually has a simple solution. The posterior assigns a probability to an event by combining our observations and beliefs about it, and can be decomposed with Bayes' rule as the product of how likely our observations are given the event (probability of observing **y** if the stimulus is **x**) and how likely the event is independent of our observations (probability of the stimulus being **x**). That is:\n",
    "\n",
    "P(**x** | **y**) ~ P(**y** | **x**) * P(**x**)\n",
    "\n",
    "where P(**y** | **x**) is called the likelihood and P(x) is called the prior.\n",
    "\n",
    "We will assume that the likelihood and the prior are multivariate Gaussian distributions. A Gaussian is characterized by two parameters: a mean vector and a covariance matrix. In the case of the likelihood, the mean of the Gaussian is given by:\n",
    "\n",
    "**mu**\\_likelihood = **B'** **x**\n",
    "\n",
    "As before, we can estimate **B** in close form with ridge regression:\n",
    "\n",
    "**B** = inv(**X**\\_training' **X**\\_training + lambda **I**) **X**\\_training' **Y**\\_training\n",
    "\n",
    "where lambda is the regularization coefficient, I is the *p* x *p* identity matrix, and *p* is the number of pixels. The covariance matrix of the likelihood is given by:\n",
    "\n",
    "**Sigma**_likelihood = diag(E[||**y** - **B'** **x**|| ^ 2]).\n",
    "\n",
    "In the case of the prior, the mean of the Gaussian is given by:\n",
    "\n",
    "**mu**\\_prior = **0** (which is a vector of zeros)\n",
    "\n",
    "The covariance matrix of the prior is given by:\n",
    "\n",
    "**Sigma**\\_prior = **X**\\_prior' * **X**\\_prior / (n - 1)\n",
    "\n",
    "where n is the length of **X**\\_prior.\n",
    "\n",
    "* Estimate **B** on the training set. Tip: Normally, you should use cross-validation to estimate lambda and Sigma_likelihood. For simplicity, you can assume that lambda = 10 ^ -6 and Sigma_likelihood = 10 ^ -3 **I**.\n",
    "* Estimate **Sigma**\\_prior. Tip: Add 10 ^ -6 to the diagonal of Sigma_prior for regularization.\n",
    "* Visualize **Sigma**\\_prior.\n",
    "\n",
    "ToThink 2: What does the visualized **Sigma**\\_prior shows? Can you also visualize the covariance of one pixel (take one in the middle) -- what does this tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-5DaPAEtc_6"
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5cTHPDuwDh"
   },
   "source": [
    "## Task 3.2\n",
    "\n",
    "Having defined the likelihood and the prior as Gaussians, we can derive the posterior by multiplying them. It turns out that the product of two Gaussians is another Gaussian, whose mean vector is given by:\n",
    "\n",
    "**mu**\\_posterior = inv(inv(**Sigma**\\_prior) + **B** inv(**Sigma**\\_likelihood) **B**') **B** * inv(**Sigma**\\_likelihood) **y**\n",
    "\n",
    "We are almost done. Recall that the reconstruction of **x** from **y** is the argument that maximizes the posterior, which we derived to be a Gaussian. We will be completely done once we answer the following question: What is the argument that maximizes a Gaussian?\n",
    "\n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".\n",
    "\n",
    "The answer is its mean vector, which is the solution of our initial problem. That is:\n",
    "\n",
    "argmax_**x** P(**x** | **y**) =  \n",
    "**mu**\\_posterior =  \n",
    "inv(inv(**Sigma**\\_prior) + **B** inv(**Sigma**\\_likelihood) **B**') **B** * inv(**Sigma**\\_likelihood) **y**\n",
    "\n",
    "ToThink 3: Why does the mean vector of the posterior distribution represent the most probable $x$ given $y$?\n",
    "\n",
    "Now, we can plug any **y** in the above equation and reconstruct the most probable **x** that could have caused it.\n",
    "\n",
    "- Reconstruct **x** from **y** in the test set.\n",
    "- Visualize the reconstructions.\n",
    "\n",
    "ToThink 4: Compare the reconstructions with the earlier reconstructions. Which one is better? Why? Can you think of ways to improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZWkvt_Ftc6F"
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF8caTCppzfK"
   },
   "source": [
    "## Task 5\n",
    "\n",
    "Quantify the reconstruction performance in terms of pixel similarity: the Pearson correlation coefficient between predicted and actual pixel values in the stimuli and reconstructions, respectively.\n",
    "\n",
    "ToThink 5: what is the downside of this metric? Can you propose a more human-centric metric to assess reconstruction performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i0EcDMLp7eo"
   },
   "outputs": [],
   "source": [
    "def pearson_correlation_coefficient(x: np.ndarray, y: np.ndarray, axis: int) -> np.ndarray:\n",
    "    r = (np.nan_to_num(stats.zscore(x)) * np.nan_to_num(stats.zscore(y))).mean(axis)\n",
    "    p = 2 * t.sf(np.abs(r / np.sqrt((1 - r ** 2) / (x.shape[0] - 2))), x.shape[0] - 2)\n",
    "    return r, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUCRzFlKqKoD"
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
